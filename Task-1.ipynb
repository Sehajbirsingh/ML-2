{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Load Data\n",
    "Import required libraries and load a subset of 20 Newsgroups data to reduce memory usage. Use categories parameter to limit categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load a subset of 20 Newsgroups data to reduce memory usage\n",
    "categories = ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Feature Extractors\n",
    "Set up CountVectorizer with max_features limit, Word2Vec with smaller vector size, and Doc2Vec with reduced dimensions. Include memory-efficient parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m count_vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Set up Word2Vec with smaller vector size\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      8\u001b[0m word2vec_model \u001b[38;5;241m=\u001b[39m Word2Vec(sentences\u001b[38;5;241m=\u001b[39m[text\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m newsgroups_train\u001b[38;5;241m.\u001b[39mdata], vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Set up Doc2Vec with reduced dimensions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mynam\\ML-2\\myenv\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mynam\\ML-2\\myenv\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mynam\\ML-2\\myenv\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mc:\\Users\\mynam\\ML-2\\myenv\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mc:\\Users\\mynam\\ML-2\\myenv\\Lib\\site-packages\\gensim\\matutils.py:1034\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 \u001b[38;5;241m&\u001b[39m set2)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1033\u001b[0m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[1;32m-> 1034\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogsumexp\u001b[39m(x):\n",
      "File \u001b[1;32mc:\\Users\\mynam\\ML-2\\myenv\\Lib\\site-packages\\gensim\\_matutils.pyx:1\u001b[0m, in \u001b[0;36minit gensim._matutils\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# Configure Feature Extractors\n",
    "\n",
    "# Set up CountVectorizer with max_features limit\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "\n",
    "# Set up Word2Vec with smaller vector size\n",
    "from gensim.models import Word2Vec\n",
    "word2vec_model = Word2Vec(sentences=[text.split() for text in newsgroups_train.data], vector_size=50, window=5, min_count=2, workers=4)\n",
    "\n",
    "# Set up Doc2Vec with reduced dimensions\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(newsgroups_train.data)]\n",
    "doc2vec_model = Doc2Vec(documents, vector_size=50, window=5, min_count=2, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Text Preprocessing\n",
    "Implement efficient text preprocessing pipeline using stop words removal and limiting vocabulary size. Use min_df to remove rare terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Text Preprocessing\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Configure CountVectorizer with stop words removal and min_df to remove rare terms\n",
    "count_vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, max_features=1000, min_df=5)\n",
    "\n",
    "# Configure TfidfVectorizer with stop words removal and min_df to remove rare terms\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, max_features=1000, min_df=5)\n",
    "\n",
    "# Fit and transform the training data using CountVectorizer\n",
    "X_train_count = count_vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test_count = count_vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "# Fit and transform the training data using TfidfVectorizer\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "# Display the shape of the transformed data\n",
    "print(\"CountVectorizer - Training data shape:\", X_train_count.shape)\n",
    "print(\"CountVectorizer - Test data shape:\", X_test_count.shape)\n",
    "print(\"TfidfVectorizer - Training data shape:\", X_train_tfidf.shape)\n",
    "print(\"TfidfVectorizer - Test data shape:\", X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Efficient Model Pipeline\n",
    "Initialize models (MultinomialNB, LogisticRegression, LinearSVC, DecisionTreeClassifier) with optimized parameters. Use smaller subset for initial testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Efficient Model Pipeline\n",
    "\n",
    "# Initialize models with optimized parameters\n",
    "models = {\n",
    "    'Multinomial Naive Bayes': MultinomialNB(alpha=0.1),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=100, solver='liblinear'),\n",
    "    'Support Vector Machine': SVC(kernel='linear', C=1),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=10)\n",
    "}\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate models using CountVectorizer features\n",
    "results_count = {}\n",
    "for name, model in models.items():\n",
    "    accuracy = evaluate_model(model, X_train_count, X_test_count, newsgroups_train.target, newsgroups_test.target)\n",
    "    results_count[name] = accuracy\n",
    "\n",
    "# Evaluate models using TfidfVectorizer features\n",
    "results_tfidf = {}\n",
    "for name, model in models.items():\n",
    "    accuracy = evaluate_model(model, X_train_tfidf, X_test_tfidf, newsgroups_train.target, newsgroups_test.target)\n",
    "    results_tfidf[name] = accuracy\n",
    "\n",
    "# Display results\n",
    "print(\"Results using CountVectorizer features:\")\n",
    "for name, accuracy in results_count.items():\n",
    "    print(f\"{name}: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nResults using TfidfVectorizer features:\")\n",
    "for name, accuracy in results_tfidf.items():\n",
    "    print(f\"{name}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Benchmark Comparison\n",
    "Execute benchmark using scikit-learn's Pipeline for memory efficiency. Implement early stopping where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Benchmark Comparison\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data to reduce memory usage\n",
    "X_train_small, _, y_train_small, _ = train_test_split(newsgroups_train.data, newsgroups_train.target, train_size=0.1, random_state=42)\n",
    "X_test_small, _, y_test_small, _ = train_test_split(newsgroups_test.data, newsgroups_test.target, train_size=0.1, random_state=42)\n",
    "\n",
    "# Define pipelines for each model and feature extractor\n",
    "pipelines = {\n",
    "    'Multinomial Naive Bayes with CountVectorizer': Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words=ENGLISH_STOP_WORDS, max_features=1000, min_df=5)),\n",
    "        ('clf', MultinomialNB(alpha=0.1))\n",
    "    ]),\n",
    "    'Logistic Regression with CountVectorizer': Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words=ENGLISH_STOP_WORDS, max_features=1000, min_df=5)),\n",
    "        ('clf', LogisticRegression(max_iter=100, solver='liblinear'))\n",
    "    ]),\n",
    "    'Support Vector Machine with CountVectorizer': Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words=ENGLISH_STOP_WORDS, max_features=1000, min_df=5)),\n",
    "        ('clf', SVC(kernel='linear', C=1))\n",
    "    ]),\n",
    "    'Decision Tree with CountVectorizer': Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words=ENGLISH_STOP_WORDS, max_features=1000, min_df=5)),\n",
    "        ('clf', DecisionTreeClassifier(max_depth=10))\n",
    "    ]),\n",
    "    'Multinomial Naive Bayes with TfidfVectorizer': Pipeline([\n",
    "        ('vect', TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, max_features=1000, min_df=5)),\n",
    "        ('clf', MultinomialNB(alpha=0.1))\n",
    "    ]),\n",
    "    'Logistic Regression with TfidfVectorizer': Pipeline([\n",
    "        ('vect', TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, max_features=1000, min_df=5)),\n",
    "        ('clf', LogisticRegression(max_iter=100, solver='liblinear'))\n",
    "    ]),\n",
    "    'Support Vector Machine with TfidfVectorizer': Pipeline([\n",
    "        ('vect', TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, max_features=1000, min_df=5)),\n",
    "        ('clf', SVC(kernel='linear', C=1))\n",
    "    ]),\n",
    "    'Decision Tree with TfidfVectorizer': Pipeline([\n",
    "        ('vect', TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, max_features=1000, min_df=5)),\n",
    "        ('clf', DecisionTreeClassifier(max_depth=10))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Evaluate pipelines\n",
    "results = {}\n",
    "for name, pipeline in pipelines.items():\n",
    "    pipeline.fit(X_train_small, y_train_small)\n",
    "    y_pred = pipeline.predict(X_test_small)\n",
    "    accuracy = accuracy_score(y_test_small, y_pred)\n",
    "    results[name] = accuracy\n",
    "\n",
    "# Display results\n",
    "print(\"Benchmark Results:\")\n",
    "for name, accuracy in results.items():\n",
    "    print(f\"{name}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results\n",
    "Create performance comparison plots using lightweight plotting libraries. Include execution time and memory usage metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Convert results to DataFrame for easier plotting\n",
    "results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy'])\n",
    "\n",
    "# Plot accuracy comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Accuracy', y='Model', data=results_df, palette='viridis')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Model')\n",
    "plt.show()\n",
    "\n",
    "# Assuming execution time and memory usage metrics are collected in a dictionary\n",
    "# Example: metrics = {'Model': ['Model1', 'Model2'], 'Execution Time': [0.5, 0.7], 'Memory Usage': [100, 150]}\n",
    "# Convert metrics to DataFrame for easier plotting\n",
    "metrics = {\n",
    "    'Model': list(results.keys()),\n",
    "    'Execution Time': [0.5, 0.7, 0.6, 0.8, 0.55, 0.75, 0.65, 0.85],  # Example values\n",
    "    'Memory Usage': [100, 150, 120, 160, 110, 140, 130, 170]  # Example values\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Plot execution time comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Execution Time', y='Model', data=metrics_df, palette='magma')\n",
    "plt.title('Model Execution Time Comparison')\n",
    "plt.xlabel('Execution Time (seconds)')\n",
    "plt.ylabel('Model')\n",
    "plt.show()\n",
    "\n",
    "# Plot memory usage comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Memory Usage', y='Model', data=metrics_df, palette='coolwarm')\n",
    "plt.title('Model Memory Usage Comparison')\n",
    "plt.xlabel('Memory Usage (MB)')\n",
    "plt.ylabel('Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Summary:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 81\u001b[0m\n\u001b[0;32m     71\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     73\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy,\n\u001b[0;32m     74\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime (s)\u001b[39m\u001b[38;5;124m'\u001b[39m: exec_time,\n\u001b[0;32m     75\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMemory (MB)\u001b[39m\u001b[38;5;124m'\u001b[39m: memory_used\n\u001b[0;32m     76\u001b[0m         })\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPerformance Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults_df\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime (s)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMemory (MB)\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Load smaller subset of data\n",
    "categories = ['alt.atheism', 'comp.graphics']  # Reduced categories\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Optimized vectorizers\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=500,  # Reduced features\n",
    "    stop_words='english',\n",
    "    min_df=0.01,\n",
    "    max_df=0.9,\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=500,  # Reduced features\n",
    "    stop_words='english',\n",
    "    min_df=0.01,\n",
    "    max_df=0.9,\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True\n",
    ")\n",
    "\n",
    "# Transform data\n",
    "X_train_count = count_vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test_count = count_vectorizer.transform(newsgroups_test.data)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "# Fast classifiers\n",
    "classifiers = {\n",
    "    'MultinomialNB': MultinomialNB(alpha=1.0),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=100, solver='liblinear', tol=0.01),\n",
    "    'LinearSVC': LinearSVC(max_iter=100, tol=0.01),\n",
    "    'DecisionTree': DecisionTreeClassifier(max_depth=10)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Benchmark with performance tracking\n",
    "for feature_name, (X_train, X_test) in {\n",
    "    'CountVectorizer': (X_train_count, X_test_count),\n",
    "    'TfidfVectorizer': (X_train_tfidf, X_test_tfidf)\n",
    "}.items():\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        start_time = time()\n",
    "        start_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024\n",
    "        \n",
    "        clf.fit(X_train, newsgroups_train.target)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(newsgroups_test.target, y_pred)\n",
    "        exec_time = time() - start_time\n",
    "        memory_used = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 - start_memory\n",
    "        \n",
    "        results.append({\n",
    "            'Model': f'{clf_name} with {feature_name}',\n",
    "            'Accuracy': accuracy,\n",
    "            'Time (s)': exec_time,\n",
    "            'Memory (MB)': memory_used\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(results_df[['Model', 'Accuracy', 'Time (s)', 'Memory (MB)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmark Results:\n",
      "       Model Accuracy   Time\n",
      "0   NB-Count    0.914  0.00s\n",
      "1   LR-Count    0.904  0.01s\n",
      "2  NB-TF-IDF    0.900  0.00s\n",
      "3  LR-TF-IDF    0.907  0.01s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "\n",
    "# Load minimal dataset\n",
    "categories = ['alt.atheism', 'comp.graphics']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, \n",
    "                                     remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, \n",
    "                                    remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Configure vectorizers\n",
    "vectorizers = {\n",
    "    'Count': CountVectorizer(max_features=500, stop_words='english'),\n",
    "    'TF-IDF': TfidfVectorizer(max_features=500, stop_words='english')\n",
    "}\n",
    "\n",
    "# Configure classifiers\n",
    "classifiers = {\n",
    "    'NB': MultinomialNB(),\n",
    "    'LR': LogisticRegression(max_iter=100, solver='liblinear')\n",
    "}\n",
    "\n",
    "# Benchmark results\n",
    "results = []\n",
    "\n",
    "for vec_name, vectorizer in vectorizers.items():\n",
    "    # Transform data\n",
    "    X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "    X_test = vectorizer.transform(newsgroups_test.data)\n",
    "    \n",
    "    for clf_name, clf in classifiers.items():\n",
    "        start_time = time()\n",
    "        clf.fit(X_train, newsgroups_train.target)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        train_time = time() - start_time\n",
    "        \n",
    "        accuracy = accuracy_score(newsgroups_test.target, y_pred)\n",
    "        results.append({\n",
    "            'Model': f'{clf_name}-{vec_name}',\n",
    "            'Accuracy': f'{accuracy:.3f}',\n",
    "            'Time': f'{train_time:.2f}s'\n",
    "        })\n",
    "\n",
    "# Print results\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(pd.DataFrame(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmark Results:\n",
      "                 Model Accuracy   Time\n",
      "0             NB-Count    0.914  0.00s\n",
      "1             LR-Count    0.904  0.02s\n",
      "2            SVM-Count    0.888  0.00s\n",
      "3   DecisionTree-Count    0.831  0.02s\n",
      "4            NB-TF-IDF    0.900  0.00s\n",
      "5            LR-TF-IDF    0.907  0.00s\n",
      "6           SVM-TF-IDF    0.912  0.02s\n",
      "7  DecisionTree-TF-IDF    0.839  0.03s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "\n",
    "# Load minimal dataset\n",
    "categories = ['alt.atheism', 'comp.graphics']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, \n",
    "                                     remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, \n",
    "                                    remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Configure vectorizers\n",
    "vectorizers = {\n",
    "    'Count': CountVectorizer(max_features=500, stop_words='english'),\n",
    "    'TF-IDF': TfidfVectorizer(max_features=500, stop_words='english')\n",
    "}\n",
    "\n",
    "# Configure classifiers\n",
    "classifiers = {\n",
    "    'NB': MultinomialNB(),\n",
    "    'LR': LogisticRegression(max_iter=100, solver='liblinear'),\n",
    "    'SVM': LinearSVC(max_iter=100, tol=0.01),\n",
    "    'DecisionTree': DecisionTreeClassifier(max_depth=50, min_samples_split=5)\n",
    "}\n",
    "\n",
    "# Benchmark results\n",
    "results = []\n",
    "\n",
    "for vec_name, vectorizer in vectorizers.items():\n",
    "    # Transform data\n",
    "    X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "    X_test = vectorizer.transform(newsgroups_test.data)\n",
    "    \n",
    "    for clf_name, clf in classifiers.items():\n",
    "        start_time = time()\n",
    "        clf.fit(X_train, newsgroups_train.target)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        train_time = time() - start_time\n",
    "        \n",
    "        accuracy = accuracy_score(newsgroups_test.target, y_pred)\n",
    "        results.append({\n",
    "            'Model': f'{clf_name}-{vec_name}',\n",
    "            'Accuracy': f'{accuracy:.3f}',\n",
    "            'Time': f'{train_time:.2f}s'\n",
    "        })\n",
    "\n",
    "# Print results\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(pd.DataFrame(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NaiveBayes...\n",
      "Training LogisticRegression...\n",
      "Training LinearSVC...\n",
      "Training DecisionTree...\n",
      "\n",
      "Results:\n",
      "        Classifier  Accuracy     Time\n",
      "        NaiveBayes  0.851449 0.015625\n",
      "LogisticRegression  0.838768 0.031632\n",
      "         LinearSVC  0.838768 0.016161\n",
      "      DecisionTree  0.645833 0.077739\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "\n",
    "# Load smaller dataset for testing\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=['alt.atheism', 'comp.graphics', 'sci.med'],\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    categories=['alt.atheism', 'comp.graphics', 'sci.med'],\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")\n",
    "\n",
    "# Feature extraction\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    min_df=5,\n",
    "    max_df=0.8,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Transform text data\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'NaiveBayes': MultinomialNB(),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=200),\n",
    "    'LinearSVC': LinearSVC(max_iter=200),\n",
    "    'DecisionTree': DecisionTreeClassifier(max_depth=20)\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "results = []\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    start = time()\n",
    "    clf.fit(X_train, newsgroups_train.target)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    duration = time() - start\n",
    "    accuracy = accuracy_score(newsgroups_test.target, y_pred)\n",
    "    results.append([name, accuracy, duration])\n",
    "\n",
    "# Display results\n",
    "df_results = pd.DataFrame(results, columns=['Classifier', 'Accuracy', 'Time'])\n",
    "print(\"\\nResults:\")\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
